<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>RISEOffline</title>
  <link rel="icon" href="https://fonts.gstatic.com/s/e/notoemoji/latest/1f305/512.gif" type="image/gif">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <div class="toc">
    <h3>Content</h3>
    <hr>
    <ul>
      <li><a href="#abstract">Abstract</a></li>
      <li><a href="#approach">Approach</a></li>
      <li><a href="#ood-recovery">OOD Recovery</a></li>
      <li><a href="#suboptimal-demos">Suboptimal Demos</a></li>
      <li><a href="#deformable-tasks">Deformable Tasks</a></li>
      <li><a href="#multitask">Multi-Task Learning</a></li>
      <li><a href="#algorithm">The Algorithm</a></li>
      <!-- <li><a href="#generalization">Generalization</a></li> -->
      <li><a href="#acknowledgements">Acknowledgements</a></li>
    </ul>
  </div>

  <div class="main-content">
    <div class="hero-text">RISE
    <img src="https://fonts.gstatic.com/s/e/notoemoji/latest/1f305/512.gif" alt="ðŸŒ…" style="display:inline-block; width:64px; height:64px; box-shadow:none; border:none; background:none; vertical-align:middle;">
    </div>
    <div class="sub-hero-text">Using Non-Expert Data to Robustify Imitation
Learning via Offline Reinforcement Learning</div>

    <!-- Add Authors -->
    <div class="authors">
      <a href="https://kevinhuang8.github.io/" target="_blank">Kevin Huang*</a>, <a href="https://rosari.ooo/" target="_blank">Rosario Scalise*</a>,
      Cleah Winston,
      Yunchu Zhang,
      Rohan Baijal, 
      Ayush Agrawal,
      Markus Grotz,
      Bryon Boots,
      Abhishek Gupta
      &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(*:equal contribution)
      <span class="affiliation">University of Washington
      <img src="https://upload.wikimedia.org/wikipedia/commons/1/17/Washington_Huskies_logo.svg" 
          alt="UW Logo" 
          style="height:1.6em; width:auto; aspect-ratio:auto; box-shadow:none; border:none; background:none; vertical-align:middle; margin-left:4px; border-radius:0;">
      </span>
      Benjamin Burchfiel,
      Hongkai Dai,
      Masha Itkina,
      Paarth Shah
      <span class="affiliation">Toyota Research Institute
      <img src="https://upload.wikimedia.org/wikipedia/commons/d/d5/Toyota_Research_Institute_Logo_Square.png"
           alt="Toyota Logo"
           style="height:3em; width:auto; aspect-ratio:auto; box-shadow:none; border:none; background:none; vertical-align:middle; margin-left:4px;">
      </span>
    </div>
    <!-- End Authors -->

    <video id="teaser-video" src="./figs/teaser.mp4" width="80%" height="auto" controls muted playsinline autoplay></video>
    <!-- Caption for Figure 1 (Teaser Video) -->
    <p class="figure-caption">
        <span style="font-variant: small-caps;">RISE</span>: Robust Imitation by Stiching from Experts demonstrates the following benefits.
    </p>

    <!-- Add Quick Links Here -->
    <div class="quick-links">
      <a href="./todo.pdf" target="_blank">[pdf]</a>
      <a href="https://github.com/UWRobotlearning" target="_blank">[arxiv]</a>
      <a href="https://github.com/UWRobotlearning">[code(soon)]</a> 
    </div>
    
    <div class="tagline" id="abstract">Abstract.</div>

    <div class="section">
        Imitation learning has proven highly effective for training robots to
        perform complex tasks from expert human demonstrations. However, it remains
        limited by reliance on high-quality, task-specific data, restricting adaptability to
        the diverse range of real-world object configurations and scenarios. In contrast,
        non-expert dataâ€”such as play data, suboptimal demonstrations, or partial task
        completionsâ€”can offer broader coverage and lower collection costs, but conven-
        tional imitation learning approaches fail to utilize this data effectively. To address
        these challenges, we show that offline reinforcement learning can be used as a
        tool to harness non-expert data to enhance the performance of imitation learning
        policies. We show that while standard offline RL approaches can be ineffective
        at actually leveraging non-expert data under sparse coverage, simple algorithmic
        modifications can allow the utilization of this data without significant additional
        assumptions. Our approach shows that broadening the support of the policy dis-
        tribution in offline RL can allow offline RL augmented imitation algorithms to
        solve tasks robustly, under sparse coverage. In manipulation tasks, these innova-
        tions significantly increase the range of states where learned policies are success-
        ful when non-expert data is incorporated. Moreover, we show that these methods
        are able to leverage all collected data, including partial or suboptimal demonstra-
        tions, to bolster task-directed policy performance, underscoring the importance of
        methods for using non-expert data for scalable and robust robot learning.
        We introduce <b>R</b>obust <b>I</b>mitation Learning by <b>S</b>titching from <b>E</b>xperts,
        or <span style="font-variant: small-caps;">RISE</span> ðŸŒ…. 
    </div> 

    <!-- =============================== APPROACH ==================================== -->
    <div class="tagline" id="approach">Approach: Robust Imitation Learning via Offline RL.</div>
    
    <div class="section">
      <p>
        <!-- TODO -->
      </p>
    </div>

    <!-- Paper Abstract Figure -->
    <a id="figure-1-img" href="figs/Teaser_Figure.png" download="Teaser_Figure.png">
      <img src="figs/Teaser_Figure.png" alt="RISE Approach Overview">
    </a>
    <p class="figure-caption">
      <b>Figure 1:</b> Overview of RISE approach showing how non-expert data is leveraged to robustify imitation learning.
    </p>

    <!-- =============================== OOD RECOVERY ==================================== -->
    <div class="tagline" id="ood-recovery">RISE Enables OOD Recovery.</div>
    
    <div class="section">
      <p>
        RISE stitches together non-expert data trajectories, such as SE2 planar pushing data, with expert data to recover from OOD states.
      </p>
    </div>

    <!-- Condensed OOD Recovery Layout -->
    <div class="condensed-recovery-layout">
      <!-- Top row: First two examples side by side -->
      <div class="recovery-row">
        <div class="recovery-item">
          <p class="recovery-description">By utilizing play data, RISE is able to recover from <b>OOD states</b>.</p>
          <div class="video-comparison-container">
            <div class="video-item">
              <video class="video-bc" src="./lampshade/Lampshade_BC_OOD.mp4" autoplay muted playsinline loop></video>
              <p class="video-caption"><b>Diffusion Policy:</b> BC fails to recover from initial states that are OOD.</p>
            </div>
            <div class="video-item">
              <video class="video-rise" src="./lampshade/LampshadeRecovery.mp4" autoplay muted playsinline loop></video>
              <p class="video-caption"><b>RISE:</b> By leveraging non-expert data, RISE is able to recover from OOD states.</p>
            </div>
          </div>
        </div>
        
        <div class="recovery-item">
          <p class="recovery-description">RISE enables recovery from cases where robot <b>misses the target</b> upon first attempt.</p>
          <div class="video-comparison-container">
            <div class="video-item">
              <video class="video-bc" src="./tipping/Tipping_Single_Disturbance_BCU.mp4" autoplay muted playsinline loop></video>
              <p class="video-caption"><b>Diffusion Policy:</b> BC is unable to recover from distrubtion shift due to the inability to utilize non-expert data effectively.</p>
            </div>
            <div class="video-item">
              <video class="video-rise" src="./tipping/Tipping_Recover_From_Failure.mp4" autoplay muted playsinline loop></video>
              <p class="video-caption"><b>RISE:</b> RISE is able to recover from robot mistakes.</p>
            </div>
          </div>
        </div>
      </div>
      
      <!-- Bottom row: Third example centered -->
      <div class="recovery-row">
        <div class="recovery-item centered">
          <p class="recovery-description">RISE enables <b>continual recovery</b> in cases where robot is subject to multiple physical perturbations.</p>
          <div class="video-comparison-container">
            <div class="video-item">
              <!-- TODO: Replace with FAILING BC Tipping -->
              <video class="video-bc" src="./tipping/Lampshade_Tipping_BC_Disturbance.mp4" autoplay muted playsinline loop></video>
              <p class="video-caption"><b>Diffusion Policy:</b> BC fails to stitch together non-expert data with expert data.</p>
            </div>
            <div class="video-item">
              <video class="video-rise" src="./tipping/Tipping_Two_Disturbance.mp4" autoplay muted playsinline loop></video>
              <p class="video-caption"><b>RISE:</b> RISE is able to continually recover from multiple disturbances.</p>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- Expandable Demo Gallery for OOD Recovery -->
    <div class="expandable-section">
      <div class="expandable-header" onclick="toggleExpandable('ood-demo-gallery')">
        <h4>Data for Lampshade Tasks</h4>
        <span class="expandable-icon" id="ood-demo-icon">â–¼</span>
      </div>
      <div class="expandable-content" id="ood-demo-gallery">
        <div class="demo-gallery">
          <div class="demo-gallery-container">
            <div class="demo-gallery-inner">
              <div class="demo-video-item">
                <video src="./lampshade/Lampshade_Expert_DATA.mp4" autoplay muted playsinline loop></video>
                <p class="demo-video-caption">Expert demos for lampshade task collected from narrow inital distribution.</p>
              </div>
              <div class="demo-video-item">
                <video src="./lampshade/Lampshade_Play_DATA.mp4" autoplay muted playsinline loop></video>
                <p class="demo-video-caption">Play data of random tabletop pushing.</p>
              </div>
              <div class="demo-video-item">
                <video src="./tipping/Tipping_DATA.mp4" autoplay muted playsinline loop></video>
                <p class="demo-video-caption">Tipping recovery demos.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- =============================== SUBOPTIMAL DEMOS ==================================== -->
    <div class="tagline" id="suboptimal-demos">RISE Leverages Suboptimal Demonstrations.</div>
    
    <div class="section">
      <p>
        Unlike most imitation learning methods, RISE is robust to suboptimal demonstrations and is able to stitch together useful segments of suboptimal demonstrations to robustify the expert. Additionally, the non-expert data increases the coverage of the policy performance.
      </p>
    </div>
    <!-- Video Gallery Section - One Leg -->
    <div class="side-by-side-videos">
      <div class="video-comparison-container">
        <div class="video-item">
          <video class="video-bc" src="./one-leg/One_Leg_BC.mp4" autoplay muted playsinline loop></video>
          <p class="video-caption"><b>Diffusion Policy:</b> BC learns to mimic suboptimal demonstrations, hence hindering task completion.</p>
        </div>
        <div class="video-item">
          <video class="video-rise" src="./one-leg/One_Leg_Insert.mp4" autoplay muted playsinline loop></video>
          <p class="video-caption"><b>RISE:</b> RISE is able to use suboptimal demos to robustify the expert and not be negatively impacted by them.</p>
        </div>
      </div>
    </div>

    <!-- Expandable Demo Gallery for Suboptimal Demos -->
    <div class="expandable-section">
      <div class="expandable-header" onclick="toggleExpandable('suboptimal-demo-gallery')">
        <h4>Data for One-Leg Insertion Task</h4>
        <span class="expandable-icon" id="suboptimal-demo-icon">â–¼</span>
      </div>
      <div class="expandable-content" id="suboptimal-demo-gallery">
        <div class="demo-gallery">
          <div class="demo-gallery-container">
            <div class="demo-gallery-inner">
              <div class="demo-video-item">
                <video src="./one-leg/One_Leg_Expert_DATA.mp4" autoplay muted playsinline loop></video>
                <p class="demo-video-caption">Expert demos for one-leg insertion task.</p>
              </div>
              <div class="demo-video-item">
                <video src="./one-leg/One_Leg_Failed_DATA.mp4" autoplay muted playsinline loop></video>
                <p class="demo-video-caption">Suboptimal demos for one-leg insertion task from an OOD inital state.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- =============================== Deformable Task ==================================== -->
    <!-- TODO: Change title -->
    <div class="tagline" id="deformable-tasks">RISE Extends to Deformable Object Tasks.</div>
    
    <div class="section">
      <p>
        RISE generalizes beyond rigid, tabletop settings to deformable cloth stacking task requiring multiple steps.
      </p>
    </div>
    <!-- Video Gallery Section - Cloth -->
    <div class="side-by-side-videos">
      <div class="video-comparison-container">
        <div class="video-item">
          <video class="video-bc" src="./cloth/Cloth_BC.mp4" autoplay muted playsinline loop></video>
          <p class="video-caption"><b>Diffusion Policy:</b> BC fails to complete the task by stacking the folded cloth..</p>
        </div>
        <div class="video-item">
          <video class="video-rise" src="./cloth/Cloth_Fold_Stack.mp4" autoplay muted playsinline loop></video>
          <p class="video-caption"><b>RISE:</b> RISE successfully stitches folding and stacking with a deformable object.</p>
        </div>
      </div>
    </div>

    <!-- Expandable Demo Gallery for Deformable Tasks -->
    <div class="expandable-section">
      <div class="expandable-header" onclick="toggleExpandable('deformable-demo-gallery')">
        <h4>Data for Cloth Stacking Task</h4>
        <span class="expandable-icon" id="deformable-demo-icon">â–¼</span>
      </div>
      <div class="expandable-content" id="deformable-demo-gallery">
        <div class="demo-gallery">
          <div class="demo-gallery-container">
            <div class="demo-gallery-inner">
              <div class="demo-video-item">
                <video src="./cloth/Cloth_Stack_DATA.mp4" autoplay muted playsinline loop></video>
                <p class="demo-video-caption">Expert demos for stacking cloth task</p>
              </div>
              <div class="demo-video-item">
                <video src="./cloth/Cloth_Fold_DATA.mp4" autoplay muted playsinline loop></video>
                <p class="demo-video-caption">Recovery demos of folding the cloth.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

    <!-- =============================== MULTI-TASK LEARNING ==================================== -->
    <div class="tagline" id="multitask">RISE Reduces the Cost of Learning New Tasks.</div>
    
    <div class="section">
      <p>
        By utilizing cheaper data that can be shared across tasks, RISE amoritzes the cost of collecting new data needed to learn new tasks.
      </p>
    </div>

    <div class="side-by-side-videos">
      <div class="video-comparison-container">
        <div class="video-item">
          <video class="video-bc" src="./base/square_bcu.mp4" autoplay muted playsinline loop></video>
          <p class="video-caption"><b>Diffusion Policy:</b> BC fails to complete the task in OOD states for square-peg and square-hook tasks.</p>
        </div>
        <div class="video-item">
          <video class="video-rise" src="./viz/rise_video_square.mp4" autoplay muted playsinline loop></video>
          <p class="video-caption"><b>RISE:</b> By sharing square play data, RISE completes the square-peg tasks in a high-coverage of inital state.</p>
        </div>
        <div class="video-item">
          <video class="video-rise" src="./amortization/hook_policy.mp4" autoplay muted playsinline loop></video>
          <p class="video-caption"><b>RISE:</b> By sharing square play data, RISE completes the square-hook task in a high-coverage of inital state.</p>
        </div>
      </div>
    </div>

    <!-- Expandable Demo Gallery for Multi-Task Learning -->
    <div class="expandable-section">
      <div class="expandable-header" onclick="toggleExpandable('multitask-demo-gallery')">
        <h4>Data for Square Tasks</h4>
        <span class="expandable-icon" id="multitask-demo-icon">â–¼</span>
      </div>
      <div class="expandable-content" id="multitask-demo-gallery">
        <div class="demo-gallery">
          <div class="demo-gallery-container">
            <div class="demo-gallery-inner">
              <div class="demo-video-item">
                <!-- TODO: ADD EXPERT PEG -->
                <video src="./amortization/square_data.mp4" autoplay muted playsinline loop></video>
                <p class="demo-video-caption">Expert demos collected from narrow inital distribution for square-peg task.</p>
              </div>
              <!-- TODO ADD EXPERT HOOK -->
              <div class="demo-video-item">
                <video src="./amortization/hook_data.mp4" autoplay muted playsinline loop></video>
                <p class="demo-video-caption">Expert demos collected from narrow inital distribution for square-hook task.</p>
              </div>
              <!-- TODO: ADD SIM PLAY -->
              <div class="demo-video-item">
                <video src="./amortization/square_play_data.mp4" autoplay muted playsinline loop></video>
                <p class="demo-video-caption">Play demos collected from wide inital distribution for square setting.</p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>

<!-- remove if we keep above -->
  <!-- VIDEO GALLERIES -->

  <!-- Video Gallery Section - TIPPING VIDEOS -->
  <!-- <div class="video-gallery-section" id="tippingGallerySection">
    <div class="video-gallery-container">
      <div class="video-gallery" id="videoGalleryTipping">
        <video class="gallery-video" src="./tipping/Tipping_DATA.mp4" autoplay muted playsinline loop></video>
        <video class="gallery-video" src="./tipping/Tipping_Recover_From_Failure.mp4" autoplay muted playsinline loop></video>
        <video class="gallery-video" src="./tipping/Tipping_Single_Disturbance_BCU.mp4" autoplay muted playsinline loop></video>
        <video class="gallery-video" src="./tipping/Tipping_Single_Disturbance.mp4" autoplay muted playsinline loop></video> -->
        <!--- Insert more videos here!!!!-->
        <!-- Add more videos as needed, ensuring they have autoplay muted loop -->
      <!-- </div>
    </div> -->
    <!-- Container for the caption AND buttons -->
    <!-- <div class="gallery-caption-container"> -->
        <!-- Move button controls INSIDE caption container -->
        <!-- <div class="gallery-nav-controls">
            <button class="gallery-nav left" id="scrollLeftBtnTraversing">&lt;</button>
            <button class="gallery-nav right" id="scrollRightBtnTraversing">&gt;</button>
        </div>
        Caption text
        <p class="figure-caption gallery-caption">
            <b>Tipping:</b> Yada yada.
        </p>
    </div>
  </div>

Video Gallery Section - Cloth VIDEOS
  <div class="video-gallery-section" id="ClothGallerySection">
    <div class="video-gallery-container">
      <div class="video-gallery" id="videoGalleryCloth">
        Videos remain here - ADD autoplay
        <video class="gallery-video" src="./cloth/Cloth_Fold_DATA.mp4" autoplay muted playsinline loop></video>
        <video class="gallery-video" src="./cloth/Cloth_Fold.mp4" autoplay muted playsinline loop></video>
        <video class="gallery-video" src="./cloth/Cloth_Stack_DATA.mp4" autoplay muted playsinline loop></video>
        <video class="gallery-video" src="./cloth/Cloth_Stack.mp4" autoplay muted playsinline loop></video> -->
        <!--- Insert more videos here!!!!-->
        <!-- Add more videos as needed, ensuring they have autoplay muted loop -->
      <!-- </div>
    </div> -->
    <!-- Container for the caption AND buttons -->
    <!-- <div class="gallery-caption-container"> -->
        <!-- Move button controls INSIDE caption container -->
        <!-- <div class="gallery-nav-controls">
            <button class="gallery-nav left" id="scrollLeftBtnTraversing">&lt;</button>
            <button class="gallery-nav right" id="scrollRightBtnTraversing">&gt;</button>
        </div>
        <p class="figure-caption gallery-caption">
            <b>Cloth:</b> Yada yada.
        </p>
    </div>
  </div> -->
<!-- End Video Gallery Section -->

<!-- =============================== ENVIRONMENT COMPARISON SECTION ==================================== -->
<div class="tagline" id="environment-comparison">RISE Outperforms BC on a Wide Range of Tasks.</div>

<div class="env-comparison-section">
  <div class="env-comparison-header">
    <button class="env-arrow" id="env-left-arrow">&#8592;</button>
    <button class="env-arrow" id="env-right-arrow">&#8594;</button>
  </div>
  <div class="env-comparison-videos">
    <div class="env-video-item">
      <video class="video-bc" id="env-video-0" src="" autoplay muted playsinline loop></video>
      <p class="env-video-caption" id="env-caption-0">Baseline 1</p>
    </div>
    <div class="env-video-item">
      <video class="video-bc" id="env-video-1" src="" autoplay muted playsinline loop></video>
      <p class="env-video-caption" id="env-caption-1">Baseline 2</p>
    </div>
    <div class="env-video-item">
      <video class="video-rise" id="env-video-2" src="" autoplay muted playsinline loop></video>
      <p class="env-video-caption" id="env-caption-2">Our Method</p>
    </div>
  </div>
</div>
<!-- =============================== END ENVIRONMENT COMPARISON SECTION ==================================== -->


    <!-- Additional video gallery for generalization examples -->
    <!-- <div class="video-gallery-section" id="generalizationGallerySection">
      <div class="video-gallery-container">
        <div class="video-gallery" id="videoGalleryGeneralization">
          <video class="gallery-video" src="./tipping/Tipping_Single_Disturbance.mp4" autoplay muted playsinline loop></video>
          <video class="gallery-video" src="./cloth/Cloth_Fold.mp4" autoplay muted playsinline loop></video>
        </div>
      </div>
      <div class="gallery-caption-container">
          <div class="gallery-nav-controls">
              <button class="gallery-nav left" id="scrollLeftBtnGeneralization">&lt;</button>
              <button class="gallery-nav right" id="scrollRightBtnGeneralization">&gt;</button>
          </div>
          <p class="figure-caption gallery-caption">
              <b>Generalization:</b> RISE successfully generalizes to new tasks and object configurations.
          </p>
      </div>
    </div> -->





    <!-- =============================== Approach Details ==================================== -->
    <div class="tagline" id="algorithm">The Algorithm.</div>
         <!-- Offline RL Section -->
    <div class="section-subtitle">Offline RL</div>
    <p>
      Offline RL is a natural method to learn policies from non-optimal data. Reward labels for human collected demonstrations on real robots
      are difficult to obtain; thus, we only assume labels of whether the offline data belongs to an expert dataset, or a non-expert dataset. 
      We label all transitions in the expert dataset with a reward of 1, and all transitions in the non-expert dataset with a reward of 0. 
      We build off of IDQL, using the following updates:
    </p>

    <p>
      $$
      \begin{align}
      \mathcal{L}_V(\psi) &= \mathbb{E}_{(s, a) \sim (\mathcal{D}_{\text{E}} \cup \mathcal{D}_{\text{NE}})} \left[ L^{\tau}_2 (Q_\phi (s, a) - V_{\psi}(s)) \right] \qquad \text{(Value Learning)} \\
      \mathcal{L}_Q(\phi) &= \mathbb{E}_{(s, a) \sim \mathcal{D}_{\text{E}}} \left[ \left( 1 + \gamma V_{\psi}(s') - Q_{\phi}(s, a)\right)^2 \right]  + \mathbb{E}_{(s, a) \sim \mathcal{D}_{\text{NE}}} \left[ \left(\gamma V_{\psi}(s') - Q_{\phi}(s, a)\right)^2 \right]\\
      \pi_B(a|s) &= \text{argmax}_{\pi} \mathbb{E}_{{s, a} \sim (\mathcal{D}_{\text{E}} \cup \mathcal{D}_{\text{NE}})}\left[\log \pi(a|s) \right] \qquad \text{(Behavior Policy Learning)} \\
      \pi^*(a|s) &= \underset{a \in \{a_1,\dots, a_K\} \sim \pi_B(a|s)}{\text{argmax}} Q_\phi(s, a) \qquad \text{(Optimal Policy Extraction)}
  
      \end{align}
      $$
    </p>

    <p>
    However, in practice, without very large datasets, the likelihood of state-overlap in the data in a continous space tends to 0, 
    resulting in stitching across overlapping states unlikely. This prevents offline RL from effectively recovering back to expert states
    from non-expert ones. While this is challenging to solve in the most general case - we base our
    practical improvements on a set of empirical findings in a robotic manipulation setting, introducing a Lipschitz penalty using spectral normalization
    along with data augmentation to improve stitching. 
    </p>

    <!-- <a id="figure-2-img" href="figs/4.png" download="4.png">
      <img src="figs/4-min.png" alt="Lipschitz Continuity">
    </a>
    <p class="figure-caption"> -->

    <!-- Improving Stitching in Offline RL -->
    <div class="section-subtitle">Improving Stitching in Offline RL</div>
    <p>
    Empirically, we observe that Q-value functions learned with expectile regression tend to be accurate and interpolate well within a neighborhood of the data, showing
    reasonable stitching behavior. The challenge comes from policy extractionâ€”while Q functions can interpolate within a neighborhood, we find that the marginal action distribution
    captured by the behavior policy and be overly conservative. This tends to be the main practical challenge in offline RL.
    </p>

    <p>
    To understand this, consider the figures below, which demonstrate a toy task where the goal is to push an object to a goal on a table. By default, on the left figure, the action distribution
    of the policy (in our case parameterized by a diffusion model) is very narrow, and the optimal action falls outside of the policy distribution (see the right figure). To remedy this, we 
    introduce a Lipschitz penalty to the behavior policy during training, which effectively widens the action distribution, improving stitching. While there are several ways to enforce
    Lipschitz continuity, we simply opt for regularizing the policy with a spectral norm penalty. Our behavior policy loss becomes:
    </p>

    <p>
      $$
      \max_\theta \mathbb{E}_{(s, a) \sim (\mathcal{D}_{\text{E}} \cup \mathcal{D}_{\text{NE}})}\left[ \log \pi_\theta(a|s) \right] + \lambda\sum_{W \in \theta} \|\sigma_{\text{max}}(W)\|^2.
      $$
    </p>

    <p>
      The effect of spectral norm regularization is shown in the figure below, significantly widening the action distribution.
    </p>

    <a id="figure-3-img" href="SpectralGraphic.jpeg" download="SpectralGraphic.jpeg">
      <img src="figs/SpectralGraphic.jpeg" alt="Effect of Spectral Norm on Action Distribution">
    </a>
    <p class="figure-caption">
      <b>Figure 2:</b> Adding a spectral norm penalty to the behavior policy widens the action distribution, improving robustness.
    </p>

     <!-- Data Augmentation Section -->
    <!-- <a id="figure-3-img" href="figs/3.png" download="3.png">
      <img src="figs/3-min.png" alt="Training Process">
    </a>
    <p class="figure-caption">
      <b>Figure 3:</b> Training process showing how Lipschitz constraints improve policy robustness.
    </p> -->

    <p>
    In addition, we apply an explicit method of widening the policy distribution by adding additional transitions to the dataset. For a given (s, a) pair, we
    select nearby transitions (s', a') such that d(s, s') < T for some threshold T and distance metric d. We then add the transition (s, a') to the dataset.
    We use Euclidean distance in the feature space of a large pretrained vision model, in our case DINOv2, as the distance metric, which has been shown to capture
    semantic similarity between images.
    </p>

    <p>
    We can also visualize the effect of RISE in the figures below. Below, we first visualize the expert and non-expert data for the square-peg task. Though they
    appear to overlap, in practice, stitching is challenging. We see that using naive offline RL, the policy takes transitions similar to those found in 
    the non-expert data, but is unable to stitch to complete the task. In contrast, RISE, which incorporates the Lipschitz penalty and data augmentation, successfully completes the task.
    </p>

    <!-- =============================== DATA VIZ ==================================== -->
    <div class="video-grid-2x2-section">
      <div class="video-grid-2x2">
        <div class="grid-video-item">
          <video src="./viz/rewards_play_render.mp4" autoplay muted playsinline loop></video>
          <p class="grid-video-caption">Non-expert data for the square-peg task.</p>
        </div>
        <div class="grid-video-item">
          <video src="./viz/rewards_play_animation.mp4" autoplay muted playsinline loop></video>
          <p class="grid-video-caption">Visualization of trajectories in the non-expert data, showing task completion.</p>
        </div>
        <div class="grid-video-item">
          <video src="./viz/rewards_expert_render.mp4" autoplay muted playsinline loop></video>
          <p class="grid-video-caption">Example of expert data for the square-peg task.</p>
        </div>
        <div class="grid-video-item">
          <video src="./viz/rewards_expert_animation.mp4" autoplay muted playsinline loop></video>
          <p class="grid-video-caption">Visualization of trajectories in the expert data, showing task completion.</p>
        </div>
      </div>
    </div>
    <!-- =============================== END DATA VIZ ==================================== -->

    <!-- =============================== COMPARISON VIDEOS ==================================== -->
    <div class="three-video-comparison-section">
      <div class="three-video-comparison-container">
        <div class="three-video-item">
          <video class="video-bc" src="./viz/nospectral_video_square.mp4" autoplay muted playsinline loop></video>
          <p class="three-video-caption">Policy trained with naive offline RL</p>
        </div>
        <div class="three-video-item">
          <video class="no-border-video" src="./viz/comparison_graph.mp4" autoplay muted playsinline loop></video>
          <p class="three-video-caption">Naive offline RL fails to stitch, resulting in task failure, while RISE successfully stitches.</p>
        </div>
        <div class="three-video-item">
          <video class="video-rise" src="./viz/rise_video_square.mp4" autoplay muted playsinline loop></video>
          <p class="three-video-caption">Policy trained with RISE</p>
        </div>
      </div>
    </div>
    <!-- =============================== END COMPARISON VIDEOS ==================================== -->
    


    <!-- =============================== ACKNOWLEDGEMENTS ==================================== -->
    <div class="tagline" id="acknowledgements">Acknowledgements.</div>
    <div class="section" style="margin-top: -5px;">
      <p>
        The authors would like to acknowledge the members of the Robot Learning Lab and the Washington Embodied Intelligence and Robotics Development Lab for helpful and informative discussions throughout the process of this research.
        The authors would also like to thank Emma Romig for robot hardware help. This research was supported by funding from Toyota Research Institute, under the University 2.0 research program.
      </p>
    </div>

    <!-- =============================== BIBTEX ==================================== -->
    <div class="bibtex-code" id="bibtex">
      <div class="bibtex-title">BibTeX</div>
      <pre><code>Insert bibtex here.
        }</code></pre>
    </div>


  

  </div> <!-- End of main-content div -->

  <div class="footer">
     Â© UW | Adapted from design by the <span style="font-variant: small-caps;">VideoMimic</span> team. Source code available <a href="https://github.com/videomimic-1/videomimic-1.github.io" target="_blank">here</a>.
  </div>

  <!-- Teaser Video Autoplay with Delay and Loop -->
  <script>
  document.addEventListener('DOMContentLoaded', function() {
    const video = document.getElementById('teaser-video');
    const loopDelay = 3000;    // 3 seconds delay before looping
    let loopTimeout;

    if (video) {
      video.muted = true;

      video.addEventListener('ended', function() {
        clearTimeout(loopTimeout); 
        loopTimeout = setTimeout(function() {
          video.currentTime = 0; 
          video.play().catch(function(error) {
            console.log('Delayed loop play prevented for teaser video:', error);
          });
        }, loopDelay);
      });

      video.addEventListener('pause', function() {
        if (!video.ended && video.currentTime > 0) {
           clearTimeout(loopTimeout);
           console.log('Teaser video: Manual pause detected, clearing loop timeout.');
        }
      });

    } else {
      console.error('Video element with ID "teaser-video" not found.');
    }
  });
  </script>

  <!-- JavaScript for Video Gallery Navigation -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const galleries = [
        {
          sectionId: 'ClothGallerySection',
          galleryInnerId: 'videoGalleryCloth',
          scrollLeftBtnId: 'scrollLeftBtnCloth',
          scrollRightBtnId: 'scrollRightBtnCloth'
        },
        {
          sectionId: 'lampshadeGallerySection',
          galleryInnerId: 'videoGalleryLampshade',
          scrollLeftBtnId: 'scrollLeftBtnLampshade',
          scrollRightBtnId: 'scrollRightBtnLampshade'
        },
        {
          sectionId: 'generalizationGallerySection',
          galleryInnerId: 'videoGalleryGeneralization',
          scrollLeftBtnId: 'scrollLeftBtnGeneralization',
          scrollRightBtnId: 'scrollRightBtnGeneralization'
        }
      ];

      galleries.forEach(galleryConfig => {
        const gallerySection = document.getElementById(galleryConfig.sectionId);
        if (!gallerySection) {
          console.error(`Gallery section with ID ${galleryConfig.sectionId} not found.`);
          return;
        }

        const galleryContainer = gallerySection.querySelector('.video-gallery-container');
        const galleryInner = document.getElementById(galleryConfig.galleryInnerId);
        const scrollLeftBtn = document.getElementById(galleryConfig.scrollLeftBtnId);
        const scrollRightBtn = document.getElementById(galleryConfig.scrollRightBtnId);

        if (galleryContainer && galleryInner && scrollLeftBtn && scrollRightBtn) {
          const scrollAmount = (galleryInner.firstElementChild?.offsetWidth || 300) + 15;

          scrollLeftBtn.addEventListener('click', () => {
            galleryContainer.scrollBy({ left: -scrollAmount, behavior: 'smooth' });
          });

          scrollRightBtn.addEventListener('click', () => {
            galleryContainer.scrollBy({ left: scrollAmount, behavior: 'smooth' });
          });

        } else {
          console.error(`Gallery elements not found for navigation setup in section ${galleryConfig.sectionId}.`);
          if (!galleryContainer) console.error('Missing element: .video-gallery-container in section ' + galleryConfig.sectionId);
          if (!galleryInner) console.error(`Missing element with ID ${galleryConfig.galleryInnerId}`);
          if (!scrollLeftBtn) console.error(`Missing element with ID ${galleryConfig.scrollLeftBtnId}`);
          if (!scrollRightBtn) console.error(`Missing element with ID ${galleryConfig.scrollRightBtnId}`);
        }
      });
    });
  </script>

  <!-- JavaScript for Expandable Sections -->
  <script>
    function toggleExpandable(sectionId) {
      const content = document.getElementById(sectionId);
      const icon = document.getElementById(sectionId.replace('-gallery', '-demo-icon'));
      
      if (content.classList.contains('expanded')) {
        content.classList.remove('expanded');
        icon.classList.remove('expanded');
      } else {
        content.classList.add('expanded');
        icon.classList.add('expanded');
      }
    }
  </script>

  <!-- JavaScript to prevent default click on specific image links -->
  <script>
    document.addEventListener('DOMContentLoaded', function() {
      const imageLinkIds = ['figure-1-img', 'figure-2-img', 'figure-3-img'];
      imageLinkIds.forEach(id => {
        const link = document.getElementById(id);
        if (link) {
          link.addEventListener('click', function(e) {
            e.preventDefault();
          });
        }
      });
    });
  </script>

<script>
  const envData = [

    {
      title: "Environment 2",
      videos: [
        { src: "./base/two_piece_bc.mp4", caption: "Base Diffusion Policy" },
        { src: "./base/two_piece_bcu.mp4", caption: "Diffusion Policy with Non-Expert Data" },
        { src: "./base/two_piece_rise.mp4", caption: "RISE" }
      ]
    },
    {
      title: "Environment 3",
      videos: [
        { src: "./base/threading_bc.mp4", caption: "Base Diffusion Policy" },
        { src: "./base/threading_bcu.mp4", caption: "Diffusion Policy with Non-Expert Data" },
        { src: "./base/threading_rise.mp4", caption: "RISE" }
      ]
    },
    {
      title: "Environment 1",
      videos: [
        { src: "./base/square_bc.mp4", caption: "Base Diffusion Policy" },
        { src: "./base/square_bcu.mp4", caption: "Diffusion Policy with Non-Expert Data" },
        { src: "./base/square_rise.mp4", caption: "RISE" }
      ]
    },

    // Add more environments as needed
  ];

  let currentEnv = 0;

  function updateEnvSection() {
    for (let i = 0; i < 3; i++) {
      document.getElementById(`env-video-${i}`).src = envData[currentEnv].videos[i].src;
      document.getElementById(`env-caption-${i}`).textContent = envData[currentEnv].videos[i].caption;
    }
  }

  document.addEventListener('DOMContentLoaded', function() {
    updateEnvSection();
    document.getElementById('env-left-arrow').onclick = function() {
      currentEnv = (currentEnv - 1 + envData.length) % envData.length;
      updateEnvSection();
    };
    document.getElementById('env-right-arrow').onclick = function() {
      currentEnv = (currentEnv + 1) % envData.length;
      updateEnvSection();
    };
  });
</script>

</body>
</html>
